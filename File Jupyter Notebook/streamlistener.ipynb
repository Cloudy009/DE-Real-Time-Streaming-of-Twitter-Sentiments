{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c98f361",
   "metadata": {
    "id": "c5d34a56"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from pyspark.ml import Pipeline,PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e322c",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size:20px; font-weight:bold \">Tạo SparkSession và cấu hình để tương tác với Kafka và MongoDB</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9714c35",
   "metadata": {
    "id": "3a93067f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6c9cbdb3-30f0-44f9-a2ae-c0dadc855965;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.0.0 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 657ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6c9cbdb3-30f0-44f9-a2ae-c0dadc855965\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/13ms)\n",
      "24/11/21 02:36:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# spark.sparkContext.stop()\n",
    "\n",
    "#Spark Session creation configured to interact with Kfka and MongoDB\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").\\\n",
    "config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-avro_2.12:3.0.0,org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").\\\n",
    "config(\"spark.mongodb.input.uri\",\"mongodb://intent-mongo-1:27017/twitter_db.tweets\").\\\n",
    "config(\"spark.mongodb.output.uri\",\"mongodb://intent-mongo-1:27017/twitter_db.tweets\").\\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e222b",
   "metadata": {},
   "source": [
    "Tạo một phiên làm việc Spark (SparkSession), cấu hình các thư viện cần thiết để làm việc với Kafka và MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9d84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f77f938e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26185e",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size:20px; font-weight:bold \">Đọc Schema từ File JSON</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88825b3f",
   "metadata": {
    "id": "5d839226"
   },
   "outputs": [],
   "source": [
    "#Read schema file and create schema of string type\n",
    "json_schema = ''\n",
    "with open(\"schema/out/tweet_schema.json\") as f:\n",
    "    new_schema = StructType.fromJson(json.load(f))\n",
    "    json_schema = new_schema.simpleString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb1ea7",
   "metadata": {},
   "source": [
    "Đọc schema JSON từ file để định nghĩa cấu trúc dữ liệu của các tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72977ae1",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size:20px; font-weight:bold \">Đọc Dữ Liệu từ Kafka</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1418cd9",
   "metadata": {
    "id": "a1728c8d"
   },
   "outputs": [],
   "source": [
    "#Read data from Kafka topic\n",
    "json_tweets = spark\\\n",
    "  .readStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"intent-kafka-1:9092\")\\\n",
    "  .option(\"subscribe\", \"tweets\")\\\n",
    "  .option(\"startingOffsets\", \"earliest\")\\\n",
    "  .load()\\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c78af4",
   "metadata": {},
   "source": [
    "Thiết lập một stream để đọc dữ liệu từ Kafka, từ topic \"twitter_demo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4aca2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a200dd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<continuation_token:string,results:array<struct<binding_values:string,bookmark_count:bigint,community_note:string,conversation_id:string,creation_date:string,expanded_url:string,extended_entities:struct<media:array<struct<additional_media_info:struct<monetizable:boolean>,display_url:string,expanded_url:string,ext_media_availability:struct<status:string>,features:struct<large:struct<faces:array<struct<h:bigint,w:bigint,x:bigint,y:bigint>>>,medium:struct<faces:array<struct<h:bigint,w:bigint,x:bigint,y:bigint>>>,orig:struct<faces:array<struct<h:bigint,w:bigint,x:bigint,y:bigint>>>,small:struct<faces:array<struct<h:bigint,w:bigint,x:bigint,y:bigint>>>>,id_str:string,indices:array<bigint>,media_key:string,media_url_https:string,original_info:struct<focus_rects:array<struct<h:bigint,w:bigint,x:bigint,y:bigint>>,height:bigint,width:bigint>,sizes:struct<large:struct<h:bigint,resize:string,w:bigint>,medium:struct<h:bigint,resize:string,w:bigint>,small:struct<h:bigint,resize:string,w:bigint>,thumb:struct<h:bigint,resize:string,w:bigint>>,type:string,url:string,video_info:struct<aspect_ratio:array<bigint>,duration_millis:bigint,variants:array<struct<bitrate:bigint,content_type:string,url:string>>>>>>,favorite_count:bigint,in_reply_to_status_id:string,language:string,media_url:array<string>,quote_count:bigint,quoted_status:string,quoted_status_id:string,reply_count:bigint,retweet:boolean,retweet_count:bigint,retweet_status:string,retweet_tweet_id:string,source:string,text:string,timestamp:bigint,tweet_id:string,user:struct<bot:boolean,category:string,creation_date:string,default_profile:boolean,default_profile_image:boolean,description:string,external_url:string,favourites_count:bigint,follower_count:bigint,following_count:bigint,has_nft_avatar:boolean,is_blue_verified:boolean,is_private:string,is_verified:boolean,listed_count:bigint,location:string,name:string,number_of_tweets:bigint,profile_banner_url:string,profile_pic_url:string,timestamp:bigint,user_id:string,username:string,verified_type:string>,video_url:array<struct<bitrate:bigint,content_type:string,url:string>>,video_view_count:string,views:bigint>>>\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra schema đọc từ tệp\n",
    "print(json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ef70192",
   "metadata": {
    "id": "9a7891ca"
   },
   "outputs": [],
   "source": [
    "# #Refine raw data red from Kafka topic\n",
    "# refined_tweets = json_tweets\\\n",
    "#         .select(from_json(\"value\", json_schema)\\\n",
    "#         .alias(\"data\"))\\\n",
    "#         .where(\"data.lang='en'and data.created_at is not null and data.text is not null\")\\\n",
    "#         .select(\"data.text\",\n",
    "#                 from_unixtime(col(\"data.timestamp_ms\")/1000,'yyyy-MM-dd HH:mm:ss').alias(\"timestamp_ms\")) #Translate milliseconds to UTC timestamp\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', '#', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', ':', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fa0ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined_tweets = json_tweets\\\n",
    "#     .select(from_json(\"value\", json_schema).alias(\"data\"))\\\n",
    "#     .selectExpr(\"data.results as results\")\\\n",
    "#     .withColumn(\"result\", explode(\"results\"))\\\n",
    "#     .where(\"result.language = 'en' and result.creation_date is not null and result.text is not null\")\\\n",
    "#     .select(\"result.text\",\n",
    "#             from_unixtime(col(\"result.timestamp\")/1000, 'yyyy-MM-dd HH:mm:ss').alias(\"timestamp_ms\"))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', '#', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "# refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', ':', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfb4ea",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size:20px; font-weight:bold \">Tiền Xử Lý Dữ Liệu</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3248748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 02:36:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Làm sạch và chuẩn hóa dữ liệu\n",
    "refined_tweets = json_tweets\\\n",
    "    .select(from_json(\"value\", json_schema).alias(\"data\"))\\\n",
    "    .select(\"data.results\")\\\n",
    "    .selectExpr(\"explode(results) as tweet\")\\\n",
    "    .select(\"tweet.*\")\\\n",
    "    .where(\"creation_date is not null and text is not null\")\\\n",
    "    .select(\n",
    "        col(\"text\"),\n",
    "        from_unixtime(col(\"timestamp\")/1000, 'yyyy-MM-dd HH:mm:ss').alias(\"timestamp_ms\")\n",
    "    )\n",
    "# Làm sạch văn bản\n",
    "refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
    "refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', '#', ''))\n",
    "refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "refined_tweets = refined_tweets.withColumn('text', regexp_replace('text', ':', ''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15beabaf",
   "metadata": {},
   "source": [
    "Chuyển đổi dữ liệu JSON từ Kafka thành cấu trúc dữ liệu định nghĩa bởi schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa41e4",
   "metadata": {},
   "source": [
    "Lọc các tweet bằng cách chọn các tweet có ngôn ngữ là tiếng Anh và có thời gian tạo và nội dung không null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219e903",
   "metadata": {},
   "source": [
    "Loại bỏ các liên kết, tài khoản, hashtag, ký hiệu RT và dấu : khỏi nội dung tweet để làm sạch dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b57c8",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size:20px; font-weight:bold \">Tải Mô Hình và Dự Đoán</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dcdf009",
   "metadata": {
    "id": "e8d5d1e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Định nghĩa thư mục\n",
    "dir = \"sentiment/\"\n",
    "\n",
    "# Kiểm tra xem thư mục có tồn tại không\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "    print(f\"Thư mục {dir} đã được tạo.\")\n",
    "\n",
    "# Tải mô hình từ thư mục\n",
    "model = PipelineModel.load(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b7c74",
   "metadata": {},
   "source": [
    "Tải mô hình phân tích cảm xúc từ thư mục sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55d57775",
   "metadata": {
    "id": "25b6824c"
   },
   "outputs": [],
   "source": [
    "def process_row(df, epoch_id):\n",
    "    \"\"\"Applies model to the df and writes data to MongoDB\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Streaming Dataframe\n",
    "    epoch_id : int\n",
    "        Unique id for each micro batch/epoch\n",
    "    \"\"\"\n",
    "    predictions = model.transform(df)\n",
    "    predictions.show()\n",
    "    predictions.select(\"timestamp_ms\",\"text\",\"prediction\").write.format(\"mongo\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6889f532",
   "metadata": {},
   "source": [
    "Định nghĩa hàm process_row để áp dụng mô hình phân tích cảm xúc lên các dữ liệu stream và lưu kết quả dự đoán vào MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5bd96",
   "metadata": {},
   "source": [
    "<span style=\"color: blue; font-size:20px; font-weight:bold \">Ghi Kết Quả và Khởi Động Stream</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef9e50",
   "metadata": {
    "id": "54813d4b",
    "outputId": "1fbb7509-f482-4a0c-8988-f3060fa1beee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 02:36:30 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                text|       timestamp_ms|      reviewTokensUf|        reviewTokens|                  cv|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Abe Uta fights ba...|1970-01-20 22:24:01|[abe, uta, fights...|[abe, uta, fights...|(71899,[21,1784,3...|(71899,[21,1784,3...|[0.01983008209077...|[0.50495735807437...|       0.0|\n",
      "|JudoJuniors 🇹🇯 ...|1970-01-21 01:07:11|[judojuniors, 🇹?...|[judojuniors, 🇹?...|(71899,[687,1784,...|(71899,[687,1784,...|[-0.1641730088018...|[0.45904868581038...|       1.0|\n",
      "|JudoJuniors 🇹🇯 ...|1970-01-21 01:07:11|[judojuniors, 🇹?...|[judojuniors, 🇹?...|(71899,[687,1784,...|(71899,[687,1784,...|[-0.1641730088018...|[0.45904868581038...|       1.0|\n",
      "|Abe Uta fights ba...|1970-01-20 22:24:01|[abe, uta, fights...|[abe, uta, fights...|(71899,[21,1784,3...|(71899,[21,1784,3...|[0.01983008209077...|[0.50495735807437...|       0.0|\n",
      "|JudoJuniors 🇹🇯 ...|1970-01-21 01:07:11|[judojuniors, 🇹?...|[judojuniors, 🇹?...|(71899,[687,1784,...|(71899,[687,1784,...|[-0.1641730088018...|[0.45904868581038...|       1.0|\n",
      "|JudoJuniors 🇹🇯 ...|1970-01-21 01:07:11|[judojuniors, 🇹?...|[judojuniors, 🇹?...|(71899,[687,1784,...|(71899,[687,1784,...|[-0.1641730088018...|[0.45904868581038...|       1.0|\n",
      "|The hour numerals...|1970-01-21 00:57:03|[the, hour, numer...|[hour, numerals, ...|(71899,[6,222,782...|(71899,[6,222,782...|[-0.4243923550875...|[0.39546617735222...|       1.0|\n",
      "|Excited to be in ...|1970-01-20 23:44:48|[excited, to, be,...|[excited, ouargla...|(71899,[86,253,59...|(71899,[86,253,59...|[0.06770676470114...|[0.51692022785160...|       0.0|\n",
      "+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 02:36:32 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Writes streaming dataframe to ForeachBatch console which ingests data to MongoDB\n",
    "refined_tweets \\\n",
    "    .writeStream \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/data\") \\\n",
    "    .foreachBatch(process_row).start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ab576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu từ MongoDB\n",
    "df = spark.read \\\n",
    "    .format(\"mongo\") \\\n",
    "    .option(\"uri\", \"mongodb://intent-mongo-1:27017/twitter_db.tweets\") \\\n",
    "    .load()\n",
    "\n",
    "# Hiển thị một số hàng dữ liệu để kiểm tra\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b372fa96",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size:30px; font-weight:bold \">Mục đích</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d46769",
   "metadata": {},
   "source": [
    "### File streamlistener xử lý dữ liệu stream từ Kafka, tiền xử lý văn bản, áp dụng mô hình phân tích cảm xúc và lưu kết quả vào MongoDB. Nó cho phép liên tục theo dõi và phân tích các tweet theo thời gian thực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57522c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "streamlistener.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
